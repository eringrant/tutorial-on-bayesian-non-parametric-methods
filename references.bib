@article{Teh2006,
abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.6738v2},
author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
doi = {10.1198/016214506000000302},
eprint = {arXiv:1210.6738v2},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {clustering,hierarchical},
number = {476},
pages = {1566--1581},
pmid = {242869700023},
title = {{Hierarchical Dirichlet Processes}},
volume = {101},
year = {2006}
}
@techreport{Frigyik2010,
abstract = {This tutorial covers the Dirichlet distribution, Dirichlet process, P{\'{o}}lya urn (and the associated Chinese restaurant process), hierarchical Dirichlet Process, and the Indian buffet process. Apart from basic properties, we describe and contrast three methods of generating samples: stick-breaking, the P{\'{o}}lya urn, and drawing gamma random variables. For the Dirichlet process we first present an informal introduction, and then a rigorous description for those more comfortable with probability theory.},
author = {Frigyik, Bela A. and Kapila, Amol and Gupta, Maya R.},
booktitle = {UWEE Technical Report},
file = {:home/erin/Downloads/UWEETR-2010-0006 (2).pdf:pdf},
isbn = {0132066920},
issn = {01987097},
pmid = {15988873},
title = {{Introduction to the Dirichlet Distribution and Related Processes}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Introduction+to+the+Dirichlet+Distribution+and+Related+Processes{\#}0},
year = {2010}
}
@misc{Y.W.Teh.2010,
author = {{Y.W. Teh.}},
booktitle = {Encyclopedia of Machine Learning},
title = {{The Dirichlet Process}},
year = {2010}
}
@book{ross2014introduction,
  title={Introduction to probability models},
  author={Ross, Sheldon M},
  year={2014},
  publisher={Academic press}
}

@article{teh2006hierarchical,
  title={Hierarchical dirichlet processes},
  author={Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
  journal={Journal of the american statistical association},
  volume={101},
  number={476},
  year={2006}
}
